{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marsvn/BNN_training/blob/main/Bayesian/tutorial_bayesian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-dCy2BFtitO"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-uncertainty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxxgwI11uZGn",
        "outputId": "f1387198-ebb2-4920-8353-1583a82c32ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-uncertainty\n",
            "  Downloading torch_uncertainty-0.7.0.post1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from torch-uncertainty) (1.0.19)\n",
            "Collecting lightning>=2.0 (from lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: torchvision>=0.16 in /usr/local/lib/python3.12/dist-packages (from torch-uncertainty) (0.23.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from torch-uncertainty) (0.8.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from torch-uncertainty) (0.13.2)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.8.0+cu126)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (4.14.1)\n",
            "Collecting pytorch-lightning (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.45.2 (from lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting hydra-core<2.0,>=1.2.0 (from lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jsonargparse<5.0,>=4.39.0 (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading jsonargparse-4.40.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>3.1 in /usr/local/lib/python3.12/dist-packages (from lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.10.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.3.0)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from lightning[pytorch-extra]>=2.0->torch-uncertainty) (13.9.4)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.16->torch-uncertainty) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.16->torch-uncertainty) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->torch-uncertainty) (2.2.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->torch-uncertainty) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->torch-uncertainty) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.12.15)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core<2.0,>=1.2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (4.9.3)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch-uncertainty) (0.17.0)\n",
            "Collecting typeshed-client>=2.8.2 (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting jsonnet>=0.21.0 (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch-uncertainty)\n",
            "  Downloading jsonnet-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (449 bytes)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->torch-uncertainty) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->torch-uncertainty) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0,>=12.3.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0,>=12.3.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.19.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]>=2.0->torch-uncertainty) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->torch-uncertainty) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->torch-uncertainty) (1.1.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>3.1->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch-uncertainty) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch-uncertainty) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->torch-uncertainty) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->torch-uncertainty) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->torch-uncertainty) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->torch-uncertainty) (2025.8.3)\n",
            "Downloading torch_uncertainty-0.7.0.post1-py3-none-any.whl (368 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.1/368.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.3-py3-none-any.whl (824 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m824.2/824.2 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.40.2-py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonnet-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jsonnet, typeshed-client, tensorboardX, lightning-utilities, jsonargparse, hydra-core, torchmetrics, bitsandbytes, pytorch-lightning, lightning, torch-uncertainty\n",
            "Successfully installed bitsandbytes-0.47.0 hydra-core-1.3.2 jsonargparse-4.40.2 jsonnet-0.21.0 lightning-2.5.3 lightning-utilities-0.15.2 pytorch-lightning-2.5.3 tensorboardX-2.6.4 torch-uncertainty-0.7.0.post1 torchmetrics-1.8.1 typeshed-client-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSNVm38PtitR"
      },
      "source": [
        "\n",
        "# Training a Bayesian Neural Network in 20 seconds\n",
        "\n",
        "In this tutorial, we will train a variational inference Bayesian Neural Network (viBNN) LeNet classifier on the MNIST dataset.\n",
        "\n",
        "## Foreword on Bayesian Neural Networks\n",
        "\n",
        "Bayesian Neural Networks (BNNs) are a class of neural networks that estimate the uncertainty on their predictions via uncertainty\n",
        "on their weights. This is achieved by considering the weights of the neural network as random variables, and by learning their\n",
        "posterior distribution. This is in contrast to standard neural networks, which only learn a single set of weights (this can be\n",
        "seen as Dirac distributions on the weights).\n",
        "\n",
        "For more information on Bayesian Neural Networks, we refer to the following resources:\n",
        "\n",
        "- Weight Uncertainty in Neural Networks [ICML2015](https://arxiv.org/pdf/1505.05424.pdf)\n",
        "- Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users [IEEE Computational Intelligence Magazine](https://arxiv.org/pdf/2007.06823.pdf)\n",
        "\n",
        "## Training a Bayesian LeNet using TorchUncertainty models and Lightning\n",
        "\n",
        "In this first part, we train a Bayesian LeNet, based on the model and routines already implemented in TU.\n",
        "\n",
        "### 1. Loading the utilities\n",
        "\n",
        "To train a BNN using TorchUncertainty, we have to load the following modules:\n",
        "\n",
        "- our TUTrainer to improve the display of our metrics\n",
        "- the model: bayesian_lenet, which lies in the torch_uncertainty.model.classification.lenet module\n",
        "- the classification training routine from torch_uncertainty.routines module\n",
        "- the Bayesian objective: the ELBOLoss, which lies in the torch_uncertainty.losses file\n",
        "- the datamodule that handles dataloaders: MNISTDataModule from torch_uncertainty.datamodules\n",
        "\n",
        "We will also need to define an optimizer using torch.optim and Pytorch's\n",
        "neural network utils from torch.nn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdaT4e3RtitT"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "from torch_uncertainty import TUTrainer\n",
        "from torch_uncertainty.datamodules import MNISTDataModule\n",
        "from torch_uncertainty.losses import ELBOLoss\n",
        "#from torch_uncertainty.models.classification.lenet import bayesian_lenet\n",
        "from torch_uncertainty.routines import ClassificationRoutine\n",
        "\n",
        "# We also define the main hyperparameters, with just one epoch for the sake of time\n",
        "BATCH_SIZE = 512\n",
        "MAX_EPOCHS = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Callable\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from torch_uncertainty.layers.bayesian import BayesConv2d, BayesLinear\n",
        "from torch_uncertainty.layers.mc_batch_norm import MCBatchNorm2d\n",
        "from torch_uncertainty.layers.packed import PackedConv2d, PackedLinear\n",
        "from torch_uncertainty.models.wrappers.batch_ensemble import BatchEnsemble\n",
        "from torch_uncertainty.models.wrappers.stochastic import StochasticModel\n",
        "\n",
        "\n",
        "\n",
        "class _LeNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        linear_layer: type[nn.Module],\n",
        "        conv2d_layer: type[nn.Module],\n",
        "        layer_args: dict,\n",
        "        activation: Callable,\n",
        "        norm: type[nn.Module],\n",
        "        groups: int,\n",
        "        dropout_rate: float,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "\n",
        "        batchnorm = False\n",
        "        if norm == nn.Identity:\n",
        "            self.norm1 = norm()\n",
        "            self.norm2 = norm()\n",
        "        elif norm == nn.BatchNorm2d or (isinstance(norm, partial) and norm.func == MCBatchNorm2d):\n",
        "            batchnorm = True\n",
        "        else:\n",
        "            raise ValueError(f\"norm must be nn.Identity or nn.BatchNorm2d. Got {norm}.\")\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.conv1 = conv2d_layer(in_channels, 6, (5, 5), groups=groups, **layer_args)\n",
        "        if batchnorm:\n",
        "            self.norm1 = norm(6)\n",
        "        self.conv_dropout = nn.Dropout2d(p=dropout_rate)\n",
        "        self.conv2 = conv2d_layer(6, 16, (5, 5), groups=groups, **layer_args)\n",
        "        if batchnorm:\n",
        "            self.norm2 = norm(16)\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.fc1 = linear_layer(256, 120, **layer_args)\n",
        "        self.fc_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc2 = linear_layer(120, 84, **layer_args)\n",
        "        self.last_fc_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc3 = linear_layer(84, num_classes, **layer_args)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.conv_dropout(self.activation(self.norm1(self.conv1(x))))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = self.conv_dropout(self.activation(self.norm2(self.conv2(out))))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = self.pooling(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc_dropout(\n",
        "            self.activation(self.fc1(out)),\n",
        "        )\n",
        "        out = self.last_fc_dropout(self.activation(self.fc2(out)))\n",
        "        return self.fc3(out)\n",
        "\n",
        "\n",
        "def _lenet(\n",
        "    stochastic: bool,\n",
        "    in_channels: int,\n",
        "    num_classes: int,\n",
        "    layer_args: dict,\n",
        "    num_samples: int = 16,\n",
        "    linear_layer: type[nn.Module] = nn.Linear,\n",
        "    conv2d_layer: type[nn.Module] = nn.Conv2d,\n",
        "    activation: Callable = nn.ReLU,\n",
        "    norm: type[nn.Module] = nn.Identity,\n",
        "    groups: int = 1,\n",
        "    dropout_rate: float = 0.0,\n",
        ") -> _LeNet | StochasticModel:\n",
        "    model = _LeNet(\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        linear_layer=linear_layer,\n",
        "        conv2d_layer=conv2d_layer,\n",
        "        activation=activation,\n",
        "        norm=norm,\n",
        "        groups=groups,\n",
        "        layer_args=layer_args,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "    if stochastic:\n",
        "        return StochasticModel(model, num_samples)\n",
        "    return model\n",
        "\n",
        "\n",
        "def lenet(\n",
        "    in_channels: int,\n",
        "    num_classes: int,\n",
        "    activation: Callable = F.relu,\n",
        "    norm: type[nn.Module] = nn.Identity,\n",
        "    groups: int = 1,\n",
        "    dropout_rate: float = 0.0,\n",
        ") -> _LeNet:\n",
        "    return _lenet(\n",
        "        stochastic=False,\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        linear_layer=nn.Linear,\n",
        "        conv2d_layer=nn.Conv2d,\n",
        "        layer_args={},\n",
        "        activation=activation,\n",
        "        norm=norm,\n",
        "        groups=groups,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "\n",
        "\n",
        "def batchensemble_lenet(\n",
        "    in_channels: int,\n",
        "    num_classes: int,\n",
        "    num_estimators: int = 4,\n",
        "    activation: Callable = F.relu,\n",
        "    norm: type[nn.Module] = nn.BatchNorm2d,\n",
        "    groups: int = 1,\n",
        "    dropout_rate: float = 0.0,\n",
        "    repeat_training_inputs: bool = False,\n",
        ") -> _LeNet:\n",
        "    model = lenet(\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        activation=activation,\n",
        "        norm=norm,\n",
        "        groups=groups,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "    return BatchEnsemble(\n",
        "        model=model,\n",
        "        num_estimators=num_estimators,\n",
        "        repeat_training_inputs=repeat_training_inputs,\n",
        "        convert_layers=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def packed_lenet(\n",
        "    in_channels: int,\n",
        "    num_classes: int,\n",
        "    num_estimators: int = 4,\n",
        "    alpha: float = 2,\n",
        "    gamma: float = 1,\n",
        "    activation: Callable = F.relu,\n",
        "    norm: type[nn.Module] = nn.Identity,\n",
        "    groups: int = 1,\n",
        "    dropout_rate: float = 0.0,\n",
        ") -> _LeNet:\n",
        "    return _lenet(\n",
        "        stochastic=False,\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        linear_layer=PackedLinear,\n",
        "        conv2d_layer=PackedConv2d,\n",
        "        norm=norm,\n",
        "        layer_args={\n",
        "            \"num_estimators\": num_estimators,\n",
        "            \"alpha\": alpha,\n",
        "            \"gamma\": gamma,\n",
        "        },\n",
        "        activation=activation,\n",
        "        groups=groups,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "\n",
        "\n",
        "def bayesian_lenet(\n",
        "    in_channels: int,\n",
        "    num_classes: int,\n",
        "    num_samples: int = 16,\n",
        "    prior_sigma_1: float | None = None,\n",
        "    prior_sigma_2: float | None = None,\n",
        "    prior_pi: float | None = None,\n",
        "    mu_init: float | None = None,\n",
        "    sigma_init: float | None = None,\n",
        "    activation: Callable = F.relu,\n",
        "    norm: type[nn.Module] = nn.Identity,\n",
        "    groups: int = 1,\n",
        "    dropout_rate: float = 0.0,\n",
        ") -> StochasticModel:\n",
        "    layers_args = {}\n",
        "    if prior_sigma_1 is not None:\n",
        "        layers_args[\"prior_sigma_1\"] = prior_sigma_1\n",
        "    if prior_sigma_2 is not None:\n",
        "        layers_args[\"prior_sigma_2\"] = prior_sigma_2\n",
        "    if prior_pi is not None:\n",
        "        layers_args[\"prior_pi\"] = prior_pi\n",
        "    if mu_init is not None:\n",
        "        layers_args[\"mu_init\"] = mu_init\n",
        "    if sigma_init is not None:\n",
        "        layers_args[\"sigma_init\"] = sigma_init\n",
        "\n",
        "    return _lenet(\n",
        "        stochastic=True,\n",
        "        num_samples=num_samples,\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        linear_layer=BayesLinear,\n",
        "        conv2d_layer=BayesConv2d,\n",
        "        norm=norm,\n",
        "        layer_args=layers_args,\n",
        "        activation=activation,\n",
        "        groups=groups,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )"
      ],
      "metadata": {
        "id": "wy1JnT2PlFSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E21tx2W6titU"
      },
      "source": [
        "### 2. Creating the necessary variables\n",
        "\n",
        "In the following, we instantiate our trainer, define the root of the datasets and the logs.\n",
        "We also create the datamodule that handles the MNIST dataset, dataloaders and transforms.\n",
        "Please note that the datamodules can also handle OOD detection by setting the `eval_ood`\n",
        "parameter to True, as well as distribution shift with `eval_shift`.\n",
        "Finally, we create the model using the blueprint from torch_uncertainty.models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cpoVlABtitV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006e50d2-f33b-4cce-d58b-d20b1099dae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: 💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = TUTrainer(accelerator=\"gpu\", devices=1, enable_progress_bar=False, max_epochs=MAX_EPOCHS)\n",
        "\n",
        "# datamodule\n",
        "root = Path(\"data\")\n",
        "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "# model\n",
        "model = bayesian_lenet(datamodule.num_channels, datamodule.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fpsiYbvtitV"
      },
      "source": [
        "### 3. The Loss and the Training Routine\n",
        "\n",
        "Then, we just define the loss to be used during training, which is a bit special and called\n",
        "the evidence lower bound. We use the hyperparameters proposed in the blitz\n",
        "library. As we are training a classification model, we use the CrossEntropyLoss\n",
        "as the negative log likelihood. We then define the training routine using the classification\n",
        "training routine from torch_uncertainty.classification. We provide the model, the ELBO\n",
        "loss and the optimizer to the routine.\n",
        "We use an Adam optimizer with a learning rate of 0.02.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho1be1GmtitW"
      },
      "outputs": [],
      "source": [
        "loss = ELBOLoss(\n",
        "    model=model,\n",
        "    inner_loss=nn.CrossEntropyLoss(),\n",
        "    kl_weight=1 / 10000,\n",
        "    num_samples=3,\n",
        ")\n",
        "\n",
        "routine = ClassificationRoutine(\n",
        "    model=model,\n",
        "    num_classes=datamodule.num_classes,\n",
        "    loss=loss,\n",
        "    optim_recipe=optim.Adam(model.parameters(), lr=2e-2),\n",
        "    is_ensemble=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK2vkU0EtitX"
      },
      "source": [
        "### 4. Gathering Everything and Training the Model\n",
        "\n",
        "Now that we have prepared all of this, we just have to gather everything in\n",
        "the main function and to train the model using our wrapper of Lightning Trainer.\n",
        "Specifically, it needs the routine, that includes the model as well as the\n",
        "training/eval logic and the datamodule.\n",
        "The dataset will be downloaded automatically in the root/data folder, and the\n",
        "logs will be saved in the root/logs folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjT4m99otitY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87475f96-7d40-4075-9caa-e6fbf0d1a453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 337kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.58MB/s]\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name                | Type                | Params | Mode \n",
            "--------------------------------------------------------------------\n",
            "0 | ood_criterion       | MaxSoftmaxCriterion | 0      | train\n",
            "1 | model               | StochasticModel     | 88.9 K | train\n",
            "2 | loss                | ELBOLoss            | 88.9 K | train\n",
            "3 | format_batch_fn     | Identity            | 0      | train\n",
            "4 | val_cls_metrics     | MetricCollection    | 0      | train\n",
            "5 | test_cls_metrics    | MetricCollection    | 0      | train\n",
            "6 | test_id_entropy     | Entropy             | 0      | train\n",
            "7 | test_id_ens_metrics | MetricCollection    | 0      | train\n",
            "8 | mixup               | Identity            | 0      | train\n",
            "--------------------------------------------------------------------\n",
            "88.9 K    Trainable params\n",
            "0         Non-trainable params\n",
            "88.9 K    Total params\n",
            "0.355     Total estimated model params size (MB)\n",
            "64        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name                | Type                | Params | Mode \n",
            "--------------------------------------------------------------------\n",
            "0 | ood_criterion       | MaxSoftmaxCriterion | 0      | train\n",
            "1 | model               | StochasticModel     | 88.9 K | train\n",
            "2 | loss                | ELBOLoss            | 88.9 K | train\n",
            "3 | format_batch_fn     | Identity            | 0      | train\n",
            "4 | val_cls_metrics     | MetricCollection    | 0      | train\n",
            "5 | test_cls_metrics    | MetricCollection    | 0      | train\n",
            "6 | test_id_entropy     | Entropy             | 0      | train\n",
            "7 | test_id_ens_metrics | MetricCollection    | 0      | train\n",
            "8 | mixup               | Identity            | 0      | train\n",
            "--------------------------------------------------------------------\n",
            "88.9 K    Trainable params\n",
            "0         Non-trainable params\n",
            "88.9 K    Total params\n",
            "0.355     Total estimated model params size (MB)\n",
            "64        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         96.540%         \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05442         \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.18119         \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.11559         \u001b[0m\u001b[35m \u001b[0m│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         2.172%          \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         2.145%          \u001b[0m\u001b[35m \u001b[0m│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.217%          \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.236%          \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        100.000%         \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.262%          \u001b[0m\u001b[35m \u001b[0m│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Complexity        \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m   flops    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         4.61 G          \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   params   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         88.85 K         \u001b[0m\u001b[35m \u001b[0m│\n",
              "└──────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          96.540%          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05442          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.18119          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.11559          </span>│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          2.172%           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          2.145%           </span>│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.217%           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.236%           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         100.000%          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.262%           </span>│\n",
              "└──────────────┴───────────────────────────┘\n",
              "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Complexity         </span>┃\n",
              "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    flops     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          4.61 G           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    params    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          88.85 K          </span>│\n",
              "└──────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test/cal/ECE': 0.0217228252440691,\n",
              "  'test/cal/aECE': 0.021445736289024353,\n",
              "  'test/cls/Acc': 0.965399980545044,\n",
              "  'test/cls/Brier': 0.05441990867257118,\n",
              "  'test/cls/NLL': 0.11558999121189117,\n",
              "  'test/sc/AUGRC': 0.002169117797166109,\n",
              "  'test/sc/AURC': 0.0023551371414214373,\n",
              "  'test/sc/Cov@5Risk': 1.0,\n",
              "  'test/sc/Risk@80Cov': 0.002624999964609742,\n",
              "  'test/cls/Entropy': 0.1811942607164383,\n",
              "  'test/cplx/flops': 4614389760.0,\n",
              "  'test/cplx/params': 88852.0,\n",
              "  'test/ens_Disagreement': 0.02333083003759384,\n",
              "  'test/ens_Entropy': 0.16966967284679413,\n",
              "  'test/ens_MI': 0.011524590663611889}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\n",
        "trainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35c3CpYotitZ"
      },
      "source": [
        "### 5. Testing the Model\n",
        "\n",
        "Now that the model is trained, let's test it on MNIST.\n",
        "Please note that we apply a reshape to the logits to determine the dimension corresponding to the ensemble\n",
        "and to the batch. As for TorchUncertainty 0.5.2, the ensemble dimension is merged with the batch dimension\n",
        "in this order (num_estimator x batch, classes).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hus-Z760titZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "1aed77a2-54bc-4cd5-e846-0176d3208612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.42421296..2.8214867].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth:  7 2 1 0\n",
            "Output logit shape (Num predictions x Batch) x Classes:  torch.Size([64, 10])\n",
            "Predicted digits:  7 2 1 0\n",
            "Std. dev. of the scores over the posterior samples 0.000 0.001 0.000 0.065\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "def imshow(img) -> None:\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "images, labels = next(iter(datamodule.val_dataloader()))\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images[:4, ...]))\n",
        "print(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n",
        "\n",
        "# Put the model in eval mode to use several samples\n",
        "model = routine.eval()\n",
        "logits = routine(images[:4, ...])\n",
        "print(\"Output logit shape (Num predictions x Batch) x Classes: \", logits.shape)\n",
        "logits = rearrange(logits, \"(m b) c -> b m c\", b=4)  # batch_size, num_estimators, num_classes\n",
        "\n",
        "# We apply the softmax on the classes then average over the estimators\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "avg_probs = probs.mean(dim=1)\n",
        "var_probs = probs.std(dim=1)\n",
        "\n",
        "predicted = torch.argmax(avg_probs, -1)\n",
        "\n",
        "print(\"Predicted digits: \", \" \".join(f\"{predicted[j]}\" for j in range(4)))\n",
        "print(\n",
        "    \"Std. dev. of the scores over the posterior samples\",\n",
        "    \" \".join(f\"{var_probs[j][predicted[j]]:.3f}\" for j in range(4)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NehWW0EWtita"
      },
      "source": [
        "Here, we show the variance of the top prediction. This is a non-standard but intuitive way to show the diversity of the predictions\n",
        "of the ensemble. Ideally, the variance should be high when the prediction is incorrect.\n",
        "\n",
        "## References\n",
        "\n",
        "- **LeNet & MNIST:** LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. [Proceedings of the IEEE](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).\n",
        "- **Bayesian Neural Networks:** Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). Weight Uncertainty in Neural Networks. [ICML 2015](https://arxiv.org/pdf/1505.05424.pdf).\n",
        "- **The Adam optimizer:** Kingma, D. P., & Ba, J. (2014). \"Adam: A method for stochastic optimization.\" [ICLR 2015](https://arxiv.org/pdf/1412.6980.pdf).\n",
        "- **The Blitz** [library](https://github.com/piEsposito/blitz-bayesian-deep-learning) (for the hyperparameters).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}